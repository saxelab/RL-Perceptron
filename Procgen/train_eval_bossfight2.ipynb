{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a959213c-83f8-424a-863f-d1d31b6ce7e9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import gym3\n",
    "from gym3 import types_np\n",
    "import numpy as np\n",
    "from procgen import ProcgenGym3Env\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ffc02f3c-6eab-49bb-b6a9-08e8fa7b2cdc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building procgen...done\n",
      "Saved episode 1.0\n",
      "Saved episode 2.0\n",
      "Saved episode 3.0\n",
      "Saved episode 4.0\n",
      "Saved episode 5.0\n",
      "Saved episode 6.0\n",
      "Saved episode 7.0\n",
      "Saved episode 8.0\n",
      "Saved episode 9.0\n",
      "Saved episode 10.0\n",
      "Saved episode 11.0\n",
      "Saved episode 13.0\n",
      "Saved episode 14.0\n",
      "Saved episode 16.0\n",
      "Saved episode 18.0\n",
      "Saved episode 20.0\n",
      "Saved episode 23.0\n",
      "Saved episode 26.0\n",
      "Saved episode 29.0\n",
      "Saved episode 33.0\n",
      "Saved episode 37.0\n",
      "Saved episode 42.0\n",
      "Saved episode 47.0\n",
      "Saved episode 53.0\n",
      "Saved episode 60.0\n",
      "Saved episode 68.0\n",
      "Saved episode 76.0\n",
      "Saved episode 86.0\n",
      "Saved episode 97.0\n",
      "Saved episode 109.0\n",
      "Saved episode 123.0\n",
      "Saved episode 139.0\n",
      "Saved episode 157.0\n",
      "Saved episode 177.0\n",
      "Saved episode 200.0\n",
      "Saved episode 225.0\n",
      "Saved episode 254.0\n",
      "Saved episode 287.0\n",
      "Saved episode 323.0\n",
      "Saved episode 365.0\n",
      "Saved episode 411.0\n",
      "Saved episode 464.0\n"
     ]
    }
   ],
   "source": [
    "# Train several agents in parallel and save weights at intervals\n",
    "\n",
    "run_name = \"test\"\n",
    "\n",
    "alpha = 1 # Learning rate. This does not influence training because training is under a random policy, so set it to one\n",
    "\n",
    "agent_healths = np.array([1,2,3,4]) # Training agent healths to use\n",
    "Nhealths = agent_healths.shape[0]\n",
    "Nagents = 10 # Number of agents to train at each health\n",
    "max_episodes = 500000 \n",
    "\n",
    "continuing = False # Set to True to continue training for longer (increase max_episodes before running)\n",
    "\n",
    "if not continuing:\n",
    "\n",
    "    save_points = np.unique(np.round(np.logspace(0,np.log10(500000),110))) # A vector of episodes to save the weights at\n",
    "    Nints = save_points.shape[0]\n",
    "    save_ind = np.ones(Nagents)\n",
    "\n",
    "    T=100 # Maximum episode length. N.B. this is currently hard-coded in the C++ code and cannot be changed by changing this constant\n",
    "    Nfeats = 6720 # Input feature dimension\n",
    "\n",
    "    env = ProcgenGym3Env(num=Nagents, env_name=\"bossfight2\", agent_health=5, use_backgrounds=False, restrict_themes=True) # N.B. the agent_health argument is irrelevant--we do not use the returns computed by the environment/cpp code\n",
    "\n",
    "\n",
    "    w = np.zeros((Nagents, Nhealths, Nfeats))\n",
    "    ws = np.zeros((Nagents, Nints+1, Nhealths, Nfeats)) \n",
    "    y = np.zeros((Nagents, T+2))\n",
    "    X = np.zeros((Nagents, Nfeats, T+2))\n",
    "    acts = np.zeros(Nagents)\n",
    "    a = np.zeros(Nagents)\n",
    "    step = np.zeros(Nagents)\n",
    "\n",
    "\n",
    "\n",
    "    total_episodes = np.zeros(Nagents)\n",
    "    successful_episodes = np.zeros((Nagents, Nhealths))\n",
    "    cumulative_rew = np.zeros(Nagents)\n",
    "\n",
    "\n",
    "while any(total_episodes <= max_episodes):\n",
    "    rew, obs, first = env.observe()\n",
    "    cumulative_rew += rew\n",
    "    \n",
    "    for i in range(Nagents):\n",
    "        if step[i] > 0 and first[i]: # First step of new episode\n",
    "            step[i] = 0\n",
    "\n",
    "            total_episodes[i] += 1\n",
    "\n",
    "            successful_episode = cumulative_rew[i] > -agent_healths # Vectorized for all agent healths\n",
    "\n",
    "            successful_episodes[i,:] += successful_episode\n",
    "\n",
    "            \n",
    "            # REINFORCE update\n",
    "            u = np.mean(y[i,:]*X[i,:,:], axis=1).T\n",
    "            w[i,:,:] = w[i,:,:] + alpha * np.outer(successful_episode, u) # Vectorized for all agent healths\n",
    "\n",
    "            cumulative_rew[i] = 0\n",
    "            \n",
    "            if any(save_points==total_episodes[i]):\n",
    "                ws[i,save_ind[i].astype(int),:,:] = w[i,:,:]\n",
    "                save_ind[i] += 1\n",
    "                if i==0:\n",
    "                    print(f\"Saved episode {total_episodes[i]}\")\n",
    "                   \n",
    "            if i==0 and total_episodes[i] % 1000 == 0:\n",
    "                print(f\"Iteration {total_episodes[i]}\")\n",
    "\n",
    "        \n",
    "        x = obs['rgb'][i,0:35,:,:].flatten()\n",
    "        X[i,:,step[0].astype(int)] = x\n",
    "        a[i] = np.random.rand()-1/2 # Pure random policy\n",
    "        \n",
    "\n",
    "        if a[i] > 0:\n",
    "            acts[i] = 0 # Left\n",
    "            y[i,step[i].astype(int)] = 1\n",
    "        else:\n",
    "            \n",
    "            acts[i]=7 # Right\n",
    "            y[i,step[i].astype(int)] = -1\n",
    "        \n",
    "        step[i] += 1\n",
    "        \n",
    "    env.act(acts) # Take actions in all envs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7991a67c-2bbb-4f7d-ad06-a8791f912228",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "np.savez(f\"rand_agents_parallel_{run_name}.npz\",ws=ws, Nagents=Nagents, Nints=Nints, Nfeats=Nfeats, agent_healths=agent_healths, save_points=save_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cfbcdc76-0aa0-491a-b948-31d7b4bde42d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1000.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 25\u001b[0m\n\u001b[1;32m     21\u001b[0m cumulative_rew \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros(Nagents)\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28many\u001b[39m(total_episodes \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m max_episodes):\n\u001b[0;32m---> 25\u001b[0m     rew, obs, first \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobserve\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m     cumulative_rew \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m rew\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(Nagents):\n",
      "File \u001b[0;32m~/miniconda3/envs/procgen/lib/python3.8/site-packages/gym3/libenv.py:345\u001b[0m, in \u001b[0;36mCEnv.observe\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    344\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mobserve\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[np\u001b[38;5;241m.\u001b[39mndarray, Dict[\u001b[38;5;28mstr\u001b[39m, np\u001b[38;5;241m.\u001b[39mndarray], np\u001b[38;5;241m.\u001b[39mndarray]:\n\u001b[0;32m--> 345\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_c_lib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlibenv_observe\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_c_env\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    346\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m    347\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_copy_ndarray(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_rew),\n\u001b[1;32m    348\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_copy_dict(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ob),\n\u001b[1;32m    349\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_copy_ndarray(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_first),\n\u001b[1;32m    350\u001b[0m     )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Evaluate agents using softmax policy\n",
    "\n",
    "eval_episodes = 1000 # Episodes per time point to use for evaluation\n",
    "alpha = 1e-6 # Learning rate to use\n",
    "\n",
    "\n",
    "env = ProcgenGym3Env(num=Nagents, env_name=\"bossfight2\", agent_health=5, use_backgrounds=False, restrict_themes=True)\n",
    "\n",
    "\n",
    "successful_episodes = np.zeros((Nagents, Nints+1, Nhealths, Nhealths)) #first Nhealth is train, second is test\n",
    "\n",
    "max_episodes = Nints*eval_episodes\n",
    "\n",
    "for j,train_ah in enumerate(agent_healths):\n",
    "    acts = np.zeros(Nagents)\n",
    "    a = np.zeros(Nagents)\n",
    "    step = np.zeros(Nagents)\n",
    "\n",
    "    total_episodes = np.zeros(Nagents)\n",
    "    \n",
    "    cumulative_rew = np.zeros(Nagents)\n",
    "\n",
    "\n",
    "    while any(total_episodes <= max_episodes):\n",
    "        rew, obs, first = env.observe()\n",
    "        cumulative_rew += rew\n",
    "\n",
    "        for i in range(Nagents):\n",
    "            if step[i] > 0 and first[i]: # First step of new episode\n",
    "                step[i] = 0\n",
    "\n",
    "                total_episodes[i] += 1\n",
    "                \n",
    "                interval = int(total_episodes[i]/eval_episodes)\n",
    "                \n",
    "                successful_episode = cumulative_rew[i] > -agent_healths # Vectorized for all agent healths\n",
    "\n",
    "                successful_episodes[i, interval, j, :] += successful_episode\n",
    "\n",
    "                cumulative_rew[i] = 0\n",
    "\n",
    "\n",
    "\n",
    "                if i==0 and total_episodes[i] % 1000 == 0:\n",
    "                    print(f\"Iteration {total_episodes[i]}\")\n",
    "\n",
    "\n",
    "            x = obs['rgb'][i,:,:,:].flatten()\n",
    "            \n",
    "            z = x.reshape((64,64,3))\n",
    "            z[28:35,28:35,:]=0 # Remove spacecraft (turns out to be essential, otherwise get strong side bias\n",
    "            z = z[0:35,:] # Remove bottom half because there's nothing there\n",
    "            x = z.flatten()\n",
    "\n",
    "            interval = int(total_episodes[i]/eval_episodes)\n",
    "            s = 1./(1+np.exp(-alpha*ws[i,interval, j,:] @ x))\n",
    "            a[i] = 2*((np.random.rand()<s) - 1/2)\n",
    "\n",
    "\n",
    "            if a[i] > 0:\n",
    "                acts[i] = 0 # Left\n",
    "            else:\n",
    "\n",
    "                acts[i]=7 # Right\n",
    "\n",
    "            step[i] += 1\n",
    "\n",
    "        env.act(acts) # Take actions in all envs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d9059a81-d5e8-4b83-b8db-bcd76add90f8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "np.savez(f\"successful_episodes_{run_name}.npz\",successful_episodes=successful_episodes, eval_episodes=eval_episodes, agent_healths=agent_healths, save_points=save_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7245fe83-11d8-4c2e-97ba-bee2f983e8d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
